{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10kdcNwQSsqNNHK_8hWFOOxASDpB0CBTs",
      "authorship_tag": "ABX9TyO0s6ZcPcPA71IjQDrP+gnn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bmathew05/MACHINE-LEARNING/blob/main/NLP_SMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SMS Spam Detection Project\n",
        "\n",
        "# This project aims to develop a robust and accurate system for detecting spam messages in SMS data. Utilizing Natural Language Processing (NLP) techniques and various machine learning algorithms, the project aims to classify incoming messages as either spam or non-spam (ham).\n",
        "\n",
        "# **Project Workflow & Highlights:**\n",
        "\n",
        "# 1. **Data Collection and Preprocessing:** The project gathers SMS data and performs essential preprocessing steps to prepare it for model training. These steps include cleaning the data by removing non-alphanumeric characters, tokenizing messages into individual words, stemming words to their root form (e.g., \"running\" to \"run\"), and removing common stop words (e.g., \"the\", \"and\") to reduce noise and focus on meaningful words.\n",
        "# 2. **Feature Extraction:** Using TF-IDF (Term Frequency-Inverse Document Frequency), the project converts text data into numerical features that can be used by machine learning models. This process helps identify important words in messages by considering their frequency and prevalence across the dataset.\n",
        "# 3. **Model Training and Evaluation:** Five different classification algorithms are trained and evaluated to identify the most effective model for spam detection. These models include K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Random Forest (RF), Decision Tree (DT), and Naive Bayes (NB). Performance is assessed using metrics such as accuracy and classification reports, providing insights into the models' ability to correctly classify spam and non-spam messages.\n",
        "\n",
        "# **Key Findings and Results:**\n",
        "\n",
        "# * **Best Performing Models:** Among the evaluated models, Random Forest (RF), Naive Bayes (NB), Support Vector Machine (SVM), and Decision Tree (DT) demonstrated superior performance in accurately classifying SMS messages.\n",
        "# * **High Accuracy:**  RF, NB, SVM, and DT achieved high accuracy scores above 90% on the test data. This indicates their effectiveness in identifying spam messages while minimizing false positives.\n",
        "# * **Effective Classification:** The classification reports further confirm the models' ability to distinguish between spam and non-spam, exhibiting good precision, recall, and F1-scores for both classes.\n",
        "# * **K-Nearest Neighbors (KNN) was the least accurate model for SMS spam detection in this project.**\n",
        "\n",
        "# **Project Goal:**\n",
        "\n",
        "# To develop a reliable and accurate SMS spam detection system that can effectively filter unwanted messages, providing a seamless and secure communication experience for users."
      ],
      "metadata": {
        "id": "YyhPpH9UVlLb",
        "outputId": "c2074d71-91c9-48da-c949-4275f63a4c9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 9) (<ipython-input-1-08f5498507f8>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-08f5498507f8>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    3. **Model Training and Evaluation:** Five different classification algorithms are trained and evaluated to identify the most effective model for spam detection. These models include K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Random Forest (RF), Decision Tree (DT), and Naive Bayes (NB). Performance is assessed using metrics such as accuracy and classification reports, providing insights into the models' ability to correctly classify spam and non-spam messages.\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WHHz0fvNhMkd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('/content/drive/MyDrive/ML DATASETS/SMS_test.csv', encoding='ISO-8859-1')\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/ML DATASETS/SMS_train.csv', encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "oZjAni3khla8",
        "outputId": "1d1f01ed-9461-45bf-b574-e5e19444b1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ML DATASETS/SMS_test.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2404eb7d71c7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ML DATASETS/SMS_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ML DATASETS/SMS_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ML DATASETS/SMS_test.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#join two df\n",
        "df = pd.concat([df1, df2],axis=0)\n",
        "df.head()\n",
        "df"
      ],
      "metadata": {
        "id": "o0riCPdSiiHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.drop('S. No.', axis=1, inplace=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RS9XA-sPiyJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "UyHdkRL2kMG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "ewGpNyoEswQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "WKocjp2xtC5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find any duplicate values exist in table\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "ZOVwBSKntMl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot Label Distribution\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "df['Label'].value_counts().plot(kind='bar')\n",
        "plt.title('Label Distribution')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s5jSQQ0ntPIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts 'Spam' to 1 and 'Non-Spam' to 0 in the 'Label' column for numerical representation.\n",
        "df['Label']=df['Label'].map({'Spam':1,'Non-Spam':0})\n",
        "df"
      ],
      "metadata": {
        "id": "WBjjqic-uJqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracts the 'Message_body' column from the dataframe 'df' and stores it in the variable 'msg'.\n",
        "msg = df['Message_body']\n",
        "msg\n"
      ],
      "metadata": {
        "id": "r5HluA2SvRyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources for text preprocessing: stopwords and punkt tokenizer models.\n",
        "import nltk\n",
        "nltk.download('stopwords')  # Downloads the stopwords dataset\n",
        "nltk.download('punkt')      # Downloads the punkt tokenizer models\n"
      ],
      "metadata": {
        "id": "-ByHd4MLvS2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes non-alphanumeric characters from 'msg' using regular expression.\n",
        "import re\n",
        "msg=msg.str.replace('[^A-Za-z0-9]',' ',regex=True)\n",
        "msg"
      ],
      "metadata": {
        "id": "zJKd7ldVwEYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# collect meaningful words above 2 letters\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "msg=msg.apply(lambda x: ' '.join([i for i in word_tokenize(x)if len(i)>=3]))\n",
        "msg"
      ],
      "metadata": {
        "id": "6TCvUYFSxcmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform text preprocessing using stemming\n",
        "# - Tokenize each text message into words.\n",
        "# - Convert each word to lowercase to ensure uniformity.\n",
        "# - Apply Porter stemming to reduce words to their root forms (e.g., \"running\" -> \"run\").\n",
        "# - Reconstruct the message by joining the stemmed words back into a string.\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "msg=msg.apply(lambda x: ' '.join([ps.stem(i.lower()) for i in word_tokenize(x)]))\n",
        "msg"
      ],
      "metadata": {
        "id": "Rta9ebRAB05m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform text preprocessing by removing stopwords\n",
        "# - Tokenize each text message into words.\n",
        "# - Remove common stopwords (e.g., 'the', 'and', 'is') from the list of words.\n",
        "# - Reconstruct the message by joining the remaining words back into a string.\n",
        "# This helps reduce noise and focus on the meaningful words in the text.\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "sw=stopwords.words('english')\n",
        "msg=msg.apply(lambda x: ' '.join([i for i in word_tokenize(x)if i not in sw]))\n",
        "msg"
      ],
      "metadata": {
        "id": "D_y14Q3PCg_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text data into numerical features using TF-IDF\n",
        "# - TF-IDF helps identify important words by considering how often they appear in a document\n",
        "#   and how common they are across all documents.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf=TfidfVectorizer()\n",
        "x=tf.fit_transform(msg)\n",
        "x"
      ],
      "metadata": {
        "id": "rddzUx_GEvIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=df['Label'].values"
      ],
      "metadata": {
        "id": "ddbxFvTgFZj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.dtype"
      ],
      "metadata": {
        "id": "zo1GoHm4Frho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=1)\n",
        "x_train"
      ],
      "metadata": {
        "id": "ZCrjl5KtFtIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate different classifiers\n",
        "# - A list of classifiers (KNN, SVM, Random Forest, Decision Tree, Naive Bayes) is defined.\n",
        "# - For each model, the following steps are performed:\n",
        "#   1. Fit the model to the training data.\n",
        "#   2. Make predictions on the test data.\n",
        "#   3. Print the accuracy and classification report to evaluate the model's performance.\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "knn=KNeighborsClassifier()\n",
        "svc=SVC()\n",
        "rfc=RandomForestClassifier()\n",
        "dtc=DecisionTreeClassifier()\n",
        "gnb=BernoulliNB()\n",
        "lst=[knn,svc,rfc,dtc,gnb]\n",
        "for i in lst:\n",
        "  print(f'Model is {i}')\n",
        "  print(\"*************\")\n",
        "  i.fit(x_train,y_train)\n",
        "  y_pred=i.predict(x_test)\n",
        "  print(f'Accuracy is {accuracy_score(y_test,y_pred)}')\n",
        "  print(f'classification report is {classification_report(y_test,y_pred)}')"
      ],
      "metadata": {
        "id": "k_HTlvbTFzJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}